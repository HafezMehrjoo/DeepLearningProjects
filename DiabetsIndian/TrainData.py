import keras.layers as layer


def train_datal(model_keras, x_train, y_train, x_test, y_test):
    model = model_keras.Sequential()
    model.add(layer.Dense(64, input_dim=8, activation='relu'))
    model.add(layer.Dense(64, activation='relu'))
    model.add(layer.Dense(64, activation='relu'))
    model.add(layer.BatchNormalization())
    model.add(layer.Dropout(rate=0.5))
    model.add(layer.Dense(64, activation='relu'))
    model.add(layer.Dense(64, activation='relu'))
    model.add(layer.BatchNormalization())
    model.add(layer.Dropout(rate=0.5))
    model.add(layer.Dense(64, activation='relu'))
    model.add(layer.Dense(64, activation='relu'))
    model.add(layer.BatchNormalization())
    model.add(layer.Dropout(rate=0.5))
    model.add(layer.Dense(32, activation='relu'))
    model.add(layer.Dense(32, activation='relu'))
    model.add(layer.BatchNormalization())
    model.add(layer.Dropout(rate=0.5))
    model.add(layer.Dense(32, activation='relu'))
    model.add(layer.BatchNormalization())
    model.add(layer.Dropout(rate=0.5))
    model.add(layer.Dense(32, activation='relu'))
    model.add(layer.Dense(32, activation='relu'))
    model.add(layer.Dropout(rate=0.5))
    model.add(layer.Dense(32, activation='relu'))
    model.add(layer.Dropout(rate=0.5))
    model.add(layer.Dense(32, activation='relu'))
    model.add(layer.Dropout(rate=0.5))
    model.add(layer.Dense(32, activation='relu'))
    model.add(layer.Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])
    history = model.fit(x_train, y_train, epochs=221, batch_size=10, validation_split=0.2)
    acc = model.evaluate(x_test, y_test)
    epochs = history.history['val_loss'].__len__()
    print('Error Percentage', 100 - (acc[1] * 100))
    return model, history, epochs
